//
//  SpeechService.swift
//  BridgyAi
//
//  Created by –†–æ–º–∞–Ω –ì–ª–∞–≤–∞—Ü–∫–∏–π on 13.12.2025.
//

import Foundation
import Combine
import AVFoundation
import Speech

protocol SpeechServiceProtocol {
    func speak(_ text: String) -> AnyPublisher<Void, Error>
    func startListening() -> AnyPublisher<String, Error>
    func stopListening()
    var isListening: Bool { get }
}

class SpeechService: NSObject, SpeechServiceProtocol, ObservableObject, AVSpeechSynthesizerDelegate {
    private let synthesizer = AVSpeechSynthesizer()
    private lazy var audioEngine: AVAudioEngine = {
        let engine = AVAudioEngine()
        return engine
    }()
    private var speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var authorizationStatus: SFSpeechRecognizerAuthorizationStatus = .notDetermined
    private var speakPromise: ((Result<Void, Error>) -> Void)?
    private var listenPromise: ((Result<String, Error>) -> Void)?
    private var currentTranscript: String = ""
    
    @Published var isListening = false
    
    override init() {
        super.init()
        synthesizer.delegate = self
        
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å —Ä–µ—á–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º
        guard SFSpeechRecognizer.authorizationStatus() != .denied else {
            self.authorizationStatus = .denied
            return
        }
        
        if let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US")), recognizer.isAvailable {
            self.speechRecognizer = recognizer
        }
        
        // –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–µ –±–ª–æ–∫–∏—Ä—É—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) { [weak self] in
            self?.requestAuthorization()
        }
    }
    
    // MARK: - AVSpeechSynthesizerDelegate
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didFinish utterance: AVSpeechUtterance) {
        speakPromise?(.success(()))
        speakPromise = nil
        
        // –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –∞—É–¥–∏–æ —Å–µ—Å—Å–∏—é –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
            do {
                try AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
            } catch {
                print("Error deactivating audio session after speech: \(error)")
            }
        }
    }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didCancel utterance: AVSpeechUtterance) {
        speakPromise?(.failure(SpeechServiceError.synthesizerUnavailable))
        speakPromise = nil
        
        // –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –∞—É–¥–∏–æ —Å–µ—Å—Å–∏—é –ø–æ—Å–ª–µ –æ—Ç–º–µ–Ω—ã
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
            do {
                try AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
            } catch {
                print("Error deactivating audio session after speech cancel: \(error)")
            }
        }
    }
    
    private func requestAuthorization() {
        authorizationStatus = SFSpeechRecognizer.authorizationStatus()
        if authorizationStatus == .notDetermined {
            SFSpeechRecognizer.requestAuthorization { [weak self] status in
                DispatchQueue.main.async {
                    self?.authorizationStatus = status
                }
            }
        }
    }
    
    func speak(_ text: String) -> AnyPublisher<Void, Error> {
        return Future { [weak self] promise in
            guard let self = self else {
                promise(.failure(SpeechServiceError.synthesizerUnavailable))
                return
            }
            
            // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å, –µ—Å–ª–∏ –æ–Ω–∞ –∞–∫—Ç–∏–≤–Ω–∞
            if self.isListening {
                self.stopListening()
            }
            
            // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–∏–Ω—Ç–µ–∑, –µ—Å–ª–∏ –æ–Ω –∞–∫—Ç–∏–≤–µ–Ω
            if self.synthesizer.isSpeaking {
                self.synthesizer.stopSpeaking(at: .immediate)
                // –í—ã–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π promise —Å –æ—à–∏–±–∫–æ–π –æ—Ç–º–µ–Ω—ã
                self.speakPromise?(.failure(SpeechServiceError.synthesizerUnavailable))
                self.speakPromise = nil
            }
            
            // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞—É–¥–∏–æ —Å–µ—Å—Å–∏—é –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
            do {
                let audioSession = AVAudioSession.sharedInstance()
                // –ò—Å–ø–æ–ª—å–∑—É–µ–º .playback –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏, —á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å –∑–∞–ø–∏—Å—å—é
                try audioSession.setCategory(.playback, mode: .spokenAudio, options: [.duckOthers])
                try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            } catch {
                promise(.failure(error))
                return
            }
            
            let utterance = AVSpeechUtterance(string: text)
            utterance.voice = AVSpeechSynthesisVoice(language: "en-US")
            utterance.rate = 0.5
            
            // –°–æ—Ö—Ä–∞–Ω—è–µ–º promise –¥–ª—è –≤—ã–∑–æ–≤–∞ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            self.speakPromise = promise
            
            self.synthesizer.speak(utterance)
        }
        .eraseToAnyPublisher()
    }
    
    func startListening() -> AnyPublisher<String, Error> {
        return Future { [weak self] promise in
            guard let self = self else {
                promise(.failure(SpeechServiceError.recognizerUnavailable))
                return
            }
            
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω
            let audioSession = AVAudioSession.sharedInstance()
            var hasMicrophonePermission = false
            
            switch audioSession.recordPermission {
            case .granted:
                hasMicrophonePermission = true
            case .denied:
                promise(.failure(SpeechServiceError.authorizationDenied))
                return
            case .undetermined:
                audioSession.requestRecordPermission { granted in
                    if granted {
                        DispatchQueue.main.async {
                            self.continueStartListening(promise: promise)
                        }
                    } else {
                        promise(.failure(SpeechServiceError.authorizationDenied))
                    }
                }
                return
            @unknown default:
                promise(.failure(SpeechServiceError.authorizationDenied))
                return
            }
            
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
            if self.authorizationStatus != .authorized {
                // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                self.authorizationStatus = SFSpeechRecognizer.authorizationStatus()
                if self.authorizationStatus != .authorized {
                    promise(.failure(SpeechServiceError.authorizationDenied))
                    return
                }
            }
            
            guard hasMicrophonePermission else {
                promise(.failure(SpeechServiceError.authorizationDenied))
                return
            }
            
            self.continueStartListening(promise: promise)
        }
        .eraseToAnyPublisher()
    }
    
    private func continueStartListening(promise: @escaping (Result<String, Error>) -> Void) {
        guard let recognizer = self.speechRecognizer,
              recognizer.isAvailable else {
            promise(.failure(SpeechServiceError.recognizerUnavailable))
            return
        }
        
        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ—Å—Å–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å (–Ω–æ –Ω–µ –æ—á–∏—â–∞–µ–º promise, –µ—Å–ª–∏ –æ–Ω —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
        let wasListening = self.isListening
        if wasListening {
            // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π promise, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
            let oldPromise = self.listenPromise
            self.listenPromise = nil
            self.isListening = false
            
            // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∞—Ä—É—é —Å–µ—Å—Å–∏—é –±–µ–∑ –≤—ã–∑–æ–≤–∞ promise
            recognitionTask?.cancel()
            recognitionTask = nil
            
            if audioEngine.isRunning {
                audioEngine.stop()
                do {
                    let inputNode = audioEngine.inputNode
                    inputNode.removeTap(onBus: 0)
                } catch {
                    print("Error removing tap: \(error)")
                }
            }
            
            recognitionRequest?.endAudio()
            recognitionRequest = nil
            
            // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –æ—Ç–º–µ–Ω—ã —Å—Ç–∞—Ä–æ–º—É promise
            oldPromise?(.failure(SpeechServiceError.recognizerUnavailable))
        }
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º promise –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –î–û –Ω–∞—á–∞–ª–∞ –∑–∞–ø–∏—Å–∏
        self.listenPromise = promise
        self.currentTranscript = ""
        
        // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞—É–¥–∏–æ —Å–µ—Å—Å–∏—é
        do {
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            self.listenPromise = nil
            promise(.failure(error))
            return
        }
        
        let request = SFSpeechAudioBufferRecognitionRequest()
        self.recognitionRequest = request
        
        do {
            let inputNode = self.audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak request] buffer, _ in
                request?.append(buffer)
            }
            
            self.audioEngine.prepare()
            try self.audioEngine.start()
            self.isListening = true
        } catch {
            self.listenPromise = nil
            self.stopListening()
            promise(.failure(error))
            return
        }
        
        self.recognitionTask = recognizer.recognitionTask(with: request) { [weak self] result, error in
            guard let self = self else { return }
            
            if let result = result {
                let transcript = result.bestTranscription.formattedString
                self.currentTranscript = transcript
                print("üìù –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç: '\(transcript)' (—Ñ–∏–Ω–∞–ª—å–Ω—ã–π: \(result.isFinal))")
                
                if result.isFinal {
                    print("‚úÖ –ü–æ–ª—É—á–µ–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: '\(transcript)'")
                    // –ü—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –≤—ã–∑—ã–≤–∞–µ–º promise –∑–¥–µ—Å—å
                    // –∏ –æ—á–∏—â–∞–µ–º –µ–≥–æ –ø–µ—Ä–µ–¥ stopListening, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–≤–æ–π–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞
                    if let promise = self.listenPromise {
                        promise(.success(transcript))
                        self.listenPromise = nil
                    }
                    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å (–Ω–æ promise —É–∂–µ –≤—ã–∑–≤–∞–Ω, –ø–æ—ç—Ç–æ–º—É –æ–Ω –Ω–µ –±—É–¥–µ—Ç –≤—ã–∑–≤–∞–Ω –ø–æ–≤—Ç–æ—Ä–Ω–æ)
                    self.stopListening()
                }
            } else if let error = error {
                print("‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: \(error)")
                // –ù–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É, –µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –æ—Ç–º–µ–Ω–∞
                let nsError = error as NSError
                if nsError.code != 216 && nsError.code != 1700 {
                    // –†–µ–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ promise –µ—â–µ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω
                    if let promise = self.listenPromise {
                        promise(.failure(error))
                        self.listenPromise = nil
                    }
                } else {
                    // –≠—Ç–æ –æ—Ç–º–µ–Ω–∞ (216 –∏–ª–∏ 1700)
                    print("‚ÑπÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ (–∫–æ–¥: \(nsError.code))")
                    // –ï—Å–ª–∏ promise –µ—â–µ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω –∏ –µ—Å—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –µ–≥–æ
                    if let promise = self.listenPromise, !self.currentTranscript.isEmpty {
                        print("‚úÖ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ø—Ä–∏ –æ—Ç–º–µ–Ω–µ: '\(self.currentTranscript)'")
                        promise(.success(self.currentTranscript))
                        self.listenPromise = nil
                    } else if let promise = self.listenPromise {
                        // –ï—Å–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –Ω–µ—Ç, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É
                        promise(.failure(SpeechServiceError.recognizerUnavailable))
                        self.listenPromise = nil
                    }
                }
                self.stopListening()
            }
        }
    }
    
    func stopListening() {
        let wasListening = isListening
        let hasPromise = listenPromise != nil
        let transcript = currentTranscript
        
        print("üõë stopListening –≤—ã–∑–≤–∞–Ω. isListening: \(wasListening), hasPromise: \(hasPromise), transcript: '\(transcript)'")
        
        // –°–Ω–∞—á–∞–ª–∞ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é —á–∞—Å—Ç—å –∑–∞–ø–∏—Å–∏
        recognitionTask?.cancel()
        recognitionTask = nil
        
        if audioEngine.isRunning {
            audioEngine.stop()
            // –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ tap
            do {
                let inputNode = audioEngine.inputNode
                inputNode.removeTap(onBus: 0)
            } catch {
                print("Error removing tap: \(error)")
            }
        }
        
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        
        // –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –∞—É–¥–∏–æ —Å–µ—Å—Å–∏—é
        do {
            try AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
        } catch {
            print("Error deactivating audio session: \(error)")
        }
        
        isListening = false
        
        // –¢–µ–ø–µ—Ä—å –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —á–µ—Ä–µ–∑ promise, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        guard hasPromise else {
            print("‚ö†Ô∏è stopListening –≤—ã–∑–≤–∞–Ω, –Ω–æ promise –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            currentTranscript = ""
            return
        }
        
        if !transcript.isEmpty {
            print("‚úÖ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —á–µ—Ä–µ–∑ promise: '\(transcript)'")
            // –í—ã–∑—ã–≤–∞–µ–º promise —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (Future —Ç—Ä–µ–±—É–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞)
            if let promise = listenPromise {
                promise(.success(transcript))
                listenPromise = nil
                currentTranscript = ""
            } else {
                print("‚ö†Ô∏è Promise –±—ã–ª –æ—á–∏—â–µ–Ω –¥–æ –≤—ã–∑–æ–≤–∞")
            }
        } else {
            print("‚ö†Ô∏è –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ø—É—Å—Ç–æ–π, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É")
            if let promise = listenPromise {
                promise(.failure(SpeechServiceError.recognizerUnavailable))
                listenPromise = nil
                currentTranscript = ""
            } else {
                print("‚ö†Ô∏è Promise –±—ã–ª –æ—á–∏—â–µ–Ω –¥–æ –≤—ã–∑–æ–≤–∞")
            }
        }
    }
}

enum SpeechServiceError: Error {
    case synthesizerUnavailable
    case recognizerUnavailable
    case authorizationDenied
}

